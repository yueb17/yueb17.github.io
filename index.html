
<head>
  <style>

    .round_icon{
/*      width: 200px;
      height: 200px;*/
      width:"15%";
      height: 200px;
      display: flex;
      border-radius: 50%;
      align-items: center;
      justify-content: center;
      overflow: hidden;
    }
      
  </style>
  
    <meta charset="utf-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Yu Yin's Homepage</title>
<!--     <link rel="icon" href="./doc/icon/icon.png"> -->
    
    <link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"></script>
    <script src="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!--<link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css">-->

</head>

 
<body>
  
  
  <!-- JavaScript plugins (requires jQuery) -->
  <script src="https://code.jquery.com/jquery.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="js/bootstrap.min.js"></script>
    
    <!--<div id="container", style='margin:1% 20% 1% 20%'>-->
    <div style="max-width:800px" class="container">

    <div id="header">
    <div class="spacer"></div>

    <br>
    <table cellpadding="20" style="font-size: 17px">
    <tr>

    
    <!--<td><img src='./doc/photo/pro.jpeg' alt="HTML tutorial" height="280" width="224"  align="left"></td>-->
    <!-- <td><img src='./img/yuyin2.jpg' class="round_icon" alt="HTML tutorial"></td> -->
    <td width="25%" height="215"><div align="center" class="STYLE90" src="">
    <div align="center"><a href="https://yin-yu.github.io/"><img src="img/yuyin.jpg" alt="Yu Yin" width="162" height="221" hspace="20" vspace="0" border="2" class="STYLE35"></a></div>
    </div></td>
    
    <td width="10%"></td>
    
    <td>  
    <h1><strong style="font-family: 'Times New Roman';">Yu Yin  &nbsp</strong> <strong style="font-family:STFangsong; font-size:30pt; "> 印宇</strong></h1>
    <br>
<!--    <font size="+1"><I>Master Student </I><br><I>Northeastern University </I><br> <br> -->
<!--   <span class="STYLE200">Ph.D. candidate</span></span></span></span><br>
              <span class="STYLE200">Department of ECE, College of Engineering<a href="https://ece.northeastern.edu/" class="STYLE189"><br>
            </a>Northeastern University, Boston,  USA</span><span class="STYLE18"><a href="https://www.northeastern.edu/" class="STYLE189"><br>
            </a></span><span class="STYLE200"><br>
            Email: yin.yu1@husky.neu.edu<br>
            </span><br>
            <span class="STYLE200">[<a href="http://yin-yu.github.io/papers/CV_YuYIN.pdf">CV</a>] [<a href="https://scholar.google.com/citations?hl=en&user=pY0_YNcAAAAJ">Google Scholar</a>] [<a href="https://github.com/YuYin1">GitHub</a>] [<a href="https://www.linkedin.com/in/yu-yin/">LinkedIn</a>]</span><br>
 -->            
  <I>Ph.D. candidate </I><br>
  <I>Department of ECE, College of Engineering<a href="https://ece.northeastern.edu/" class="STYLE189"> </I><br>
    <I>Northeastern University, Boston, USA </I><br> <br>
    
    <a class="email" href="mailto:yin.yu1@husky.neu.edu" style="color: #333;">
      <i class="fa fa-envelope" aria-hidden="true"></i>
      &nbsp;&nbsp; Email (yin.yu1@husky.neu.edu)
    </a><br>
    </center>

    <a class="menu" href="https://github.com/YuYin1" style="color: #333;">
      <i class="fa fa-github" aria-hidden="true"></i>
      &nbsp;&nbsp; GitHub
    </a><br>
    
    <a class="menu" href="https://www.linkedin.com/in/yu-yin/" style="color: #333;">
      <i class="fa fa-linkedin" aria-hidden="true"></i>
      &nbsp;&nbsp; Linkedin
    </a>
    
<!--     <a class="email" href="mailto:zhehu.94@gmail.com">[Email]</a>
    <a class="menu" href="https://www.linkedin.com/in/zhe-derek-hu-380b62131/">[Linkedin]</a>
    <a class="menu" href="https://github.com/Derekkk">[Github]</a>
    <a class="menu" href="doc/photo/Resume_ZheHu.pdf">[CV]</a> -->

      
    </td>
    </tr>   
    </table>
    
  <br>
  <table width="712" border="0" align="left" cellspacing="3" bordercolor="#999999">
  <!-- <table width="715" border="0" align="left" cellspacing="4" bordercolor="#999999"> -->
    <tbody><tr bordercolor="#333333">
      <th width="19%" height="30" scope="col"><a href="https://yin-yu.github.io/#biography" class="STYLE212">Biography</a></th>
      <th width="20%" scope="col"><div align="center"><span class="STYLE217" style="font-size: 16"><b style="mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Education">Education</a></b></span></div></th>
      <th width="22%" scope="col"><div align="center"><span class="STYLE217" style="font-size: 16"><b style="mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Publications">Publications</a></b></span></div></th>
      <th width="20%" scope="col"><div align="center"><span class="STYLE217" style="font-size: 16"><b style="mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Activities">Activities</a></span></div></th>
      <th width="19%" scope="col"><div align="center"><a href="https://yin-yu.github.io/#Awards" class="STYLE212">Awards</a></div></th>
    </tr>
  </tbody></table>
  <!-- <table width="715" border="0" align="left" cellspacing="4" bordercolor="#999999">
    <tbody><tr bordercolor="#333333">
      <th width="111" height="30" scope="col"><a href="https://yin-yu.github.io/#biography" class="STYLE212">Biography</a></th>
      <th width="123" scope="col"><div align="center"><span class="STYLE217" style="font-size: 16"><b style="mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Education">Education</a></b></span></div></th>
      <th width="142" scope="col"><div align="center"><span class="STYLE217" style="font-size: 16"><b style="mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Publications">Publications</a></b></span></div></th>
      <th width="123" scope="col"><div align="center"><span class="STYLE217" style="font-size: 16"><b style="mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Activities">Activities</a></span></div></th>
      <th width="88" scope="col"><div align="center"><a href="https://yin-yu.github.io/#Awards" class="STYLE212">Awards</a></div></th>
    </tr>
  </tbody></table> -->
  <p><br>

  </div> <!-- end header -->
<hr/>
<div id="main">
<div id="maincontent">

<div class="content">
  <h4 class="text-primary" id="news"><font color = 'red';><I>What's New</I></font></h4>
    [2019.12] We release the code for JASRNet (accepted by AAAI 2020) at <a href="https://github.com/YuYin1/JASRNet">Github_JASRNet</a>.<br>
    [2019.11] We submit two papers to CVPR'20.<br>
    [2019.11] Our paper is accepted by AAAI'20.<br>
    [2019.09] We submit two papers to FG'20.<br>
    [2019.08] Our paper is accepted by Expert Systems With Applications (IF: 4.292).<br>
    [2019.02] Our paper is accepted by IEEE Journal of Translational Engineering in Healthand Medicine (JTEHM).<br>
    [2018.10] Our paper is accepted by IEEE Journal of Translational Engineering in Healthand Medicine (JTEHM).<br>
    [2018.06] Our paper is accepted by IJCAI Workshop'18.<br>
    [2017.10] Our paper is accepted by IEEE-NIH Special Topics Conference on Healthcare Innovations and Point-of-Care Technologies (HI-POCT'17).<br>
    <!-- <p>[July 2019] <span class="glyphicon glyphicon-plane"></span>&nbsp;  Travel to Florence, Italy - ACL 2019 <br> <p> -->

        
<hr />
</div>

<h2 id="Biography" class="top_main_heading"><span style=:'Times New Roman';">Biography</span></h2>
    <p>
        I am a first-year Ph.D. student at Department of Electrical & Computer Engineering, <a href="https://www.northeastern.edu/">Northeastern University</a>, and work with Prof. <a href="http://www1.ece.neu.edu/~yunfu/">Yun (Raymond) Fu</a> in the <a href="https://web.northeastern.edu/smilelab/">SMILE</a> Lab. Before that I received my master degree in Electrical & Computer Engineering from Northeastern University in Dec. 2018, and B.E degree from School of Electronic Engineering, <a href="http://english.whut.edu.cn/">Wuhan University of Technology</a>, China, in Jul. 2016. From Fall 2016, I was a member of the <a href="https://web.northeastern.edu/ostadabbas/">Augmented Cogniton (AClab)</a>, under the supervision of my advisor Prof. <a href="https://web.northeastern.edu/ostadabbas/teams/sarah-ostadabbas/">Sarah Ostadabbas</a>.

        My research interest broadly includes image processing (i.e., super-resolution, face generation), visual recognition (i.e., face recognition, pose estimation, face alignment, and emotion recognition), and biosignal processing. <br> <br>
        <!-- I was the recipient of the Best Student Paper Award at IEEE International Conference on Visual Communication and Image Processing (VCIP) in 2015. -->

        <hr/>
    </p>

<h2 id="Education" class="top_main_heading"><span style=:'Times New Roman';">Education</span></h2>
  <ul>
    <li>
      01/2019 - Now: Ph.D. in Computer Engineering, <a href="https://www.northeastern.edu/">Northeastern University</a>, Boston, USA
    </li>
    <li>
      09/2016 - 12/2018: M.S. in Electrical and Computer Engineering, <a href="https://www.northeastern.edu/">Northeastern University</a>, Boston, USA
    </li>
    <li>
      09/2012 - 07/2016: B.E. in Electrical and Information Engineering, <a href="http://english.whut.edu.cn/">Wuhan University of Technology</a>, Wuhan, China
    </li>
  </ul>
<hr />


<h2 id="Publications" class="top_main_heading"><span style=:'Times New Roman';">Publications</span></h2>
  <ul>
    <li>Preprints: </li>
    <!-- <a href="./paper/aaai2020_joint/YuYin_Joint_Super-Resolution_and_Alignment_of_Tiny_Faces_AAAI2020.pdf">Joint Super-Resolution and_Alignment of Tiny Faces</a> -->
      <ul>
        <li>SuperFace: From Low-resolution Image to High-resolution Frontal Face Sythesis (first author submission).</br></li>
      </ul>
      <ul>
        <li>Dual-Attention GAN for Large-Pose Face Frontalization (first author submission).</br></li>
      </ul>
      <ul>
        <li>Unity of Opposites: A Holistic Approach to Semi-supervised Domain Adaptation (collaborating with others).</br></li>
      </ul>
      <ul>
        <li>Long-Short Dual-Side AutoEncoder for Human Motion Segmentation (collaborating with others).</br></li>
      </ul>
  </ul>

  <ul>
    <li>Joint Super-Resolution and Alignment of Tiny Faces</li>
    <!-- <a href="./paper/aaai2020_joint/YuYin_Joint_Super-Resolution_and_Alignment_of_Tiny_Faces_AAAI2020.pdf">Joint Super-Resolution and_Alignment of Tiny Faces</a> -->
    <b>Yu Yin</b>, Joseph P. Robinson, Yulun Zhang, and Yun Fu</br>
    <span style="font-style: italic;"> Proceedings of the AAAI Conference on Artificial Intelligence</span> (<b>AAAI'20</b>).<br/>
    <a href="https://arxiv.org/abs/1911.08566" class="label label-success">arXiv</a>
    <a data-toggle="modal" href="#absaaai20" class="label label-primary">Abstract</a>
    <!-- <a href="./paper/" class="label label-info">Supplementary</a> -->
    <a href="https://github.com/YuYin1/JASRNet" class="label label-warning">Code</a>
    <!-- <a href="https://summarise.github.io/" class="label label-default">BibTex Demo</a> -->
        <!-- Modal -->
        <div class="modal fade" id="absaaai20" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
            <div class="modal-content">
              <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                <h4 class="modal-title" style="text-align: center"><strong>Joint Super-Resolution and_Alignment of Tiny Faces.</strong></h4>
              </div>
              <div class="modal-body">
                  <div align="center" style="padding: 0 15px 0">
                    <img src='./paper/aaai2020_joint/figs.pdf' alt="paper model" style="width: 100%">
                  </div>
                  </br>   
                  <div style="padding: 0 30px 0">
        Super-resolution (SR) and landmark localization of tiny facesare highly correlated tasks. On the one hand, landmark localization could obtain higher accuracy with faces of highresolution (HR). On the other hand, face SR would benefit from prior knowledge of facial attributes such as landmarks. Thus, we propose a joint alignment and SR network to simultaneously detect facial landmarks and super-resolve tiny faces. More specifically, a shared deep encoder is applied to extract features for both tasks by leveraging complementary information. To exploit representative power of the hierarchical encoder, intermediate layers of a shared feature extraction module are fused to form efficient feature representations. The fused features are then fed to task-specific modules to detect landmarks and super-resolve face images in parallel. Extensive experiments demonstrate that the proposed model significantly outperforms the state-of-the-art in both landmark localization and SR of faces. We show a large improveme*nt for landmark localization of tiny faces (i.e., 16 × 16). Furthermore, the proposed framework yields comparable results for landmark localization on low-resolution (LR) faces (i.e., 64 × 64) to existing methods on HR (i.e., 256 × 256). As for SR, the proposed method recovers sharper edges and more details from LR face images than other state-of-the-art methods, which we demonstrate qualitatively and quantitatively.
                  </div>
              </div>
            </div>
        </div><!-- /.modal-dialog -->
  </ul>
  
  <ul>
    <li>Facial Expression and Peripheral Physiology Fusion to Decode Individualized Affective Experience</a></li>
    <b>Yu Yin</b>, Mohsen Nabian, Miolin Fan, ChunAn Chou, Maria Gendron, and Sarah Ostadabbas</br>
    <span style="font-style: italic;">Affective Computing Workshop of the International Joint Conferences on Artificial Intelligence (<b>IJCAI Workshop'18</b>). </span><br/>
    <a href="https://arxiv.org/abs/1811.07392" class="label label-success">arXiv</a>
    <a data-toggle="modal" href="#absjtehm19inbed" class="label label-primary">Abstract</a>
    <a href="https://github.com/ostadabbas/3d-facial-landmark-detection-and-tracking" class="label label-warning">Code</a>
    <a href="./paper/ijcai2018_expression/Facial-Expression-Tracking.mov" class="label label-default">Demo</a>
    <a href="./paper/ijcai2018_expression/AffCom_IJCAI2018_Presentation.pdf" class="label label-info">Slides</a>
        <!-- Modal -->
        <div class="modal fade" id="absjtehm19inbed" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
            <div class="modal-content">
              <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                <h4 class="modal-title" style="text-align: center"><strong>Facial Expression and Peripheral Physiology Fusion to Decode Individualized Affective Experience.</strong></h4>
              </div>
              <div class="modal-body">
                  <div align="center" style="padding: 0 15px 0">
                    <img src='./paper/ijcai2018_expression/frame.png' alt="paper model" style="width: 100%">
                  </div>
                  </br>   
                  <div style="padding: 0 30px 0">
                    Affective experience prediction using different input modalities such as facial expression or physiological signals has received substantial research attention in recent years. However, most studies ignore the fact that people may have different resting patterns (both facially and in peripheral physiology) and rarely focus on concurrent analysis of these modalities. In this paper, we present a multimodal approach to simultaneously analyze facial movements and several peripheral physiological signals to decode individualized affective experiences under positive and negative emotional contexts. We propose a person-specific recurrence network to quantify the dynamics present in the person’s facial movement and physiological data. Facial movement is represented using a robust head vs. 3D face landmark localization and tracking approach, and physiological data are processed by extracting known attributes related to affective experience. The dynamical coupling between different input modalities is then assessed through the extraction of several complex recurrent network metrics. Inference models are then trained using these metrics as features to predict individual’s affective experience in a given context, after their resting dynamics are excluded from their response. We validated our approach using a dataset consists of a set of 48 videos recorded from 12 individuals while watching emotion-eliciting video stimuli. The affective experience prediction results signified that our multimodal fusion method improves the prediction accuracy up to 19% when compared to the prediction using only one or a subset of the input modalities. Furthermore, we gained prediction improvement for affective experience by considering the effect of individualized resting dynamics.
                  </div>
              </div>
        </div><!-- /.modal-dialog -->
  </ul>

  <ul>
    <li>A Biosignal-Specific Processing Tool for Machine Learning and Pattern Recognition</a></li>
    <b>Yu Yin</b>, Mohsen Nabian, Athena Nouhi, and Sarah Ostadabbas</br>
    <span style="font-style: italic;">IEEE-NIH Special Topics Conference on Healthcare Innovations and Point-of-Care Technologies (<b>HI-POCT'17</b>). </span><br/>
    <a href="./paper/hipoct2017_bioTool/HI_POCT_2017_Yu.pdf" class="label label-success">PDF</a>
    <a data-toggle="modal" href="#absjtehm19inbed" class="label label-primary">Abstract</a>
    <a href="https://www.mathworks.com/matlabcentral/fileexchange/64013-biosignal-specific-processing-bio-sp-tool" class="label label-warning">Matlab Code</a>
    <a href="./paper/hipoct2017_bioTool/Poster_HI_POCT_2017.pdf" class="label label-info">Poster</a>
    <!-- <a href="./paper/hipoct2017_bioTool/Poster_HI_POCT_2017.pdf" class="label label-default">Poster</a> -->
        <!-- Modal -->
        <div class="modal fade" id="absjtehm19inbed" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
            <div class="modal-content">
              <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                <h4 class="modal-title" style="text-align: center"><strong>A Biosignal-Specific Processing Tool for Machine Learning and Pattern Recognition.</strong></h4>
              </div>
              <div class="modal-body">
                  <div align="center" style="padding: 0 15px 0">
                    <img src='./paper/hipoct2017_bioTool/frame.png' alt="paper model" style="width: 100%">
                  </div>
                  </br>   
                  <div style="padding: 0 30px 0">
                    Electrocardiogram (ECG), Electrodermal Activity (EDA), Electromyogram (EMG) and Impedance Cardiography (ICG) are among physiological signals widely used in various biomedical applications including health tracking, sleep quality assessment, early disease detection/diagnosis and human affective state recognition. This paper presents the development of a biosignal-specific processing and feature extraction tool for analyzing these physiological signals according to the state-ofthe-art studies reported in the scientific literature. This tool is intended to assist researchers in machine learning and pattern recognition to extract feature matrix from these bio-signals automatically and reliably. In this paper, we provided the algorithms used for the signal-specific filtering and segmentation as well as extracting features that have been shown highly relevant to a better category discrimination in an intended application. This tool is an open-source software written in MATLAB and made compatible with MathWorks Classification Learner app for further classification purposes such as model training, cross-validation scheme farming, and classification result computation.
                  </div>
              </div>
            </div>
        </div><!-- /.modal-dialog -->
  </ul>

  <ul>
    <li>In-Bed Pose Estimation: Deep Learning with Shallow Dataset</a></li>
    Shuangjun Liu, <b>Yu Yin</b>, and Sarah Ostadabbas</br>
    <span style="font-style: italic;">IEEE Journal of Translational Engineering in Health and Medicine</span> (<b>JTEHM</b>), vol. 7, no. 1, pp. 1-12, Jan. 2019.<br/>
    <a href="https://arxiv.org/abs/1711.01005" class="label label-success">arXiv</a>
    <a data-toggle="modal" href="#absjtehm19inbed" class="label label-primary">Abstract</a>
    <!-- <a href="doc/bib/2019_acl.bib" class="label label-info">Supplementary</a> -->
    <a href="https://github.com/ostadabbas/in-bed-pose-estimation" class="label label-warning">Code</a>
    <a href="https://web.northeastern.edu/ostadabbas/2017/09/01/a-vision-based-system-for-in-bed-posture-tracking/" class="label label-default">Project</a>
        <!-- Modal -->
        <div class="modal fade" id="absjtehm19inbed" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
            <div class="modal-content">
              <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                <h4 class="modal-title" style="text-align: center"><strong>In-Bed Pose Estimation: Deep Learning with Shallow Dataset.</strong></h4>
              </div>
              <div class="modal-body">
                  <div align="center" style="padding: 0 15px 0">
                    <img src='./paper/jiehm2019_pose/frame.jpg' alt="paper model" style="width: 100%">
                  </div>
                  </br>   
                  <div style="padding: 0 30px 0">
        This paper presents a robust human posture and body parts detection method under a specific application scenario, in-bed pose estimation. Although human pose estimation for various computer vision (CV) applications has been studied extensively in the last few decades, yet in-bed pose estimation using camera-based vision methods has been ignored by the CV community because it is assumed to be identical to the general purpose pose estimation methods. However, in-bed pose estimation has its own specialized aspects and comes with specific challenges including the notable differences in lighting conditions throughout a day and also having different pose distribution from the common human surveillance viewpoint. In this paper, we demonstrate that these challenges significantly lessen the effectiveness of existing general purpose pose estimation models. In order to address the lighting variation challenge, infrared selective (IRS) image acquisition technique is proposed to provide uniform quality data under various lighting conditions. In addition, to deal with unconventional pose perspective, a 2-end histogram of oriented gradient (HOG) rectification method is presented. Deep learning framework proves to be the most effective model in human pose estimation, however the lack of large public dataset for in-bed poses prevents us from using a large network from scratch. In this work, we explored the idea of employing a pre-trained convolutional neural network (CNN) model trained on large public datasets of general human poses and fine-tuning the model using our own shallow (limited in size and different in perspective and color) in-bed IRS dataset. We developed an IRS imaging system and collected IRS image data from several realistic life-size mannequins in a simulated hospital room environment. A pre-trained CNN called convolutional pose machine (CPM) was repurposed for in-bed pose estimation by fine-tuning its specific intermediate layers. Using the HOG rectification method, the pose estimation performance of CPM significantly improved by 26.4% in PCK0.1 (probability of correct keypoint) criteria compared to the model without such rectification. Even testing with only well aligned in-bed pose images, our fine-tuned model still surpassed the traditionally-tuned CNN by another 16.6% increase in pose estimation accuracy.
                  </div>
              </div>
        </div><!-- /.modal-dialog -->
  </ul>

  <ul>
    <li>Analysis of Multimodal Physiological Signals Within and Across Individuals to Predict Psychological Threat vs. Challenge</a></li>
    Aya Khalaf, Mohsen Nabian, Miaolin Fan, <b>Yu Yin</b>, Jolie Wormwood, Erika Siegel, Karen S. Quigley, Lisa Feldman Barrett, Murat Akcakaya, Chun-An Chou, and Sarah Ostadabbas</br>
    <span style="font-style: italic;">Expert Systems With Applications (<b>ESWA</b>), vol. 140, Feb. 2020. </span><br/>
    <a href="https://psyarxiv.com/96djs/" class="label label-success">psyarXiv</a>
    <a data-toggle="modal" href="#absjtehm19inbed" class="label label-primary">Abstract</a>
    <!-- <a href="doc/bib/2019_acl.bib" class="label label-info">Supplementary</a> -->
    <!-- <a href="https://github.com/ostadabbas/in-bed-pose-estimation" class="label label-warning">Code</a> -->
    <!-- <a href="https://web.northeastern.edu/ostadabbas/2017/09/01/a-vision-based-system-for-in-bed-posture-tracking/" class="label label-default">Project</a> -->
        <!-- Modal -->
        <div class="modal fade" id="absjtehm19inbed" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
            <div class="modal-content">
              <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                <h4 class="modal-title" style="text-align: center"><strong>Analysis of Multimodal Physiological Signals Within and Across Individuals to Predict PsychologicalThreat vs. Challenge.</strong></h4>
              </div>
              <div class="modal-body">
                  <div align="center" style="padding: 0 15px 0">
                    <img src='./paper/eswa2020_threat/frame.png' alt="paper model" style="width: 100%">
                  </div>
                  </br>   
                  <div style="padding: 0 30px 0">
                    In this study, we aimed to investigate individual and group-level variations in physiological responding across a series of motivated performance tasks that vary in difficulty. The proposed approach is motivated by documented individual differences in physiological responses observed in motivated performance tasks, such that we first focus on individual differences in physiological responses rather than group-level comparisons. Then, through our analysis of individuals we identify sub-groups (i.e., clusters) of individuals that share common physiological patterns across tasks of varying difficulty and we perform across-subject analysis within each cluster. This is distinct from existing studies which typically do not examine individual vs. subgroup-specific patterns of physiological activity. Such an approach enables us to identify patterns in physiological responses that can be used to predict self-reported judgments of challenge vs. threat with higher accuracy in each subgroup compared to an analysis that includes the entire sample population as a single group. We employed data from an existing experiment in which participants completed three mental arithmetic tasks of increasing difficulty during which different modalities of physiological data were collected. Analyses revealed three subgroups of participants who shared common features that best differentiated their within-individual physiological response patterns across tasks. Support vector machine (SVM) classifiers were trained using both shared features within each group and all computed features to predict challenge vs. threat states. Results showed that, the within-group classification model using group common features achieved higher self-report prediction accuracy compared to an alternative model trained on data from all participants without feature selection.
                  </div>
              </div>
        </div><!-- /.modal-dialog -->
  </ul>

  <ul>
    <li>An Open-Source Feature Extraction Tool for the Analysis of Peripheral Physiological Data</a></li>
    Mohsen Nabian, <b>Yu Yin</b>, Jolie Wormwood, Karen S. Quigley, Lisa F. Barrett, and Sarah Ostadabbas</br>
    <span style="font-style: italic;">IEEE Journal of Translational Engineering in Health and Medicine</span> (<b>JTEHM</b>), vol. 6, Oct. 2019.<br/>
    <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6231905/pdf/jtehm-ostadabbas-2878000.pdf" class="label label-success">PDF</a>
    <a data-toggle="modal" href="#absjtehmbiosp" class="label label-primary">Abstract</a>
    <!-- <a href="doc/bib/2019_acl.bib" class="label label-info">Supplementary</a> -->
    <a href="https://www.mathworks.com/matlabcentral/fileexchange/64013-biosignal-specific-processing-bio-sp-tool" class="label label-warning">Matlab Code</a>
    <!-- <a href="https://web.northeastern.edu/ostadabbas/2017/09/01/a-vision-based-system-for-in-bed-posture-tracking/" class="label label-default">Project</a> -->
        <!-- Modal -->
        <div class="modal fade" id="absjtehmbiosp" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
            <div class="modal-content">
              <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                <h4 class="modal-title" style="text-align: center"><strong>An Open-Source Feature Extraction Tool for the Analysis of Peripheral Physiological Data.</strong></h4>
              </div>
              <div class="modal-body">
                  <div align="center" style="padding: 0 15px 0">
                    <img src='./paper/jiehm2019_bioTool/frame.png' alt="paper model" style="width: 100%">
                  </div>
                  </br>   
                  <div style="padding: 0 30px 0">
                    Electrocardiogram (ECG), Electrodermal Activity (EDA), Electromyogram (EMG), continuous Blood Pressure (BP) and Impedance Cardiography (ICG) are among the physiological signals widely used in various biomedical applications including health tracking, sleep quality assessment, early disease detection/diagnosis, telemedicine and human affective state recognition. This paper presents the development of a biosignal-specific tool for processing and feature extraction of these physiological signals according to state-of-the-art studies reported in the scientific literature and feedback received from field experts. This tool is intended to assist researchers in affective computing, machine learning, and pattern recognition to extract the physiological features from these biosignals automatically and reliably. In this paper, we provide algorithms for signal-specific quality checking, filtering, and segmentation as well as the extraction of features that have been shown to be highly relevant to category discrimination in biomedical and affective computing applications. This tool is an open-source software written in MATLAB and a graphical user interface (GUI) is also provided for the convenience of the users. The GUI is compatible with MathWorks Classification Learner app for further *classification purposes such as model training, cross-validation scheme farming, and classification result computation.
                  </div>
              </div>
        </div><!-- /.modal-dialog -->
  </ul>

<hr />


<h2 id="Awards" class="top_main_heading"><span style=:'Times New Roman';">Awards</span></h2>
  <ul>
    <li>AAAI Travel Grant, 2020</li>
    <li>PhD Network Travel Grant, Northeastern University, USA, 2019</li>
    <li>Excellent Bachelor Thesis of Wuhan University of Technology, 2016</li>
    <li>Scholarship of Wuhan University of Technology, 2013, 2014, 2015</li>
  </ul>
<hr/>
  
<h2 id="Activities" class="top_main_heading"><span style=:'Times New Roman';">Activities</span></h2>
  <ul> 
    <li><strong>Teaching Assistance</strong>
    <ul> 
      <li>TA of Computer Vision, 2018 Fall</li>
    </ul>
  </ul> 
  <ul> 
    <li><strong>Reviewers</strong>
    <ul> 
      <li>IEEE Transactions on Image Processing</li>
      <li>IEEE Transactions on Neural Networks and Learning Systems</li>
    </ul>
  </ul> 

<hr />


<p><span class="STYLE189 STYLE263">Created on 2017/10/01. Last update : 2019/12/2</span></p>
<div style='margin:1% 30% 1% 30%'>
  <!-- <p>    
  <a href="https://clustrmaps.com/site/1b0h9"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=GiDrPWoG3lMZDBc9YjT9O7Dih0JJe5hgvTCgWflrrzs&cl=ffffff" /></a>
  </p></body></html> -->
  <center>
  <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=GiDrPWoG3lMZDBc9YjT9O7Dih0JJe5hgvTCgWflrrzs&cl=ffffff&w=a"></script>
  </center>
</div>


<br>
<br>



<!--<h3>More Projects</h3>-->

</div> <!-- end main content section -->
</div>

</div> <!-- end container -->
</body>

</html>
